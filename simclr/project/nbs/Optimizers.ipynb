{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.vision.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=untar_data(URLs.MNIST_TINY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#42) [Path('/home/fast/.fastai/data/imagenette-160.tgz'),Path('/home/fast/.fastai/data/imagenette-160'),Path('/home/fast/.fastai/data/mnist.pkl.gz'),Path('/home/fast/.fastai/data/imagenette.tgz'),Path('/home/fast/.fastai/data/imagenette'),Path('/home/fast/.fastai/data/danbooru2018'),Path('/home/fast/.fastai/data/horse2zebra'),Path('/home/fast/.fastai/data/Selfie-dataset'),Path('/home/fast/.fastai/data/oxford-iiit-pet'),Path('/home/fast/.fastai/data/planet_tiny')...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path('/home/fast/.fastai/data').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db=DataBlock((ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(),\n",
    "                   get_y=parent_label,batch_tfms=aug_transforms(do_flip=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls=db.dataloaders(path,bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x648 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is basic setup, we will do this every time we change the code to reset all the involved variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,labels = dls.one_batch()\n",
    "x,labels=x.cpu(),labels.cpu()\n",
    "m = nn.Sequential(nn.Conv2d(3,32,7,3,3),nn.Flatten(),nn.Linear(3200,2)) #very basic pytorch model, just linear operation, no activation, so not a deep model\n",
    "l = nn.CrossEntropyLoss()\n",
    "lr=0.1\n",
    "opt=SGD(m.parameters(),lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=m(x)\n",
    "loss=l(pred,labels)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurSGD:\n",
    "    def __init__(self,params,lr):\n",
    "        self.params,self.lr=params,lr\n",
    "    def step(self):\n",
    "        updated_params=[]\n",
    "        for p in self.params:\n",
    "            updated_params.append(p.add(-self.lr*p.grad)) #important part!!!\n",
    "        return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important part above is the p.add(-self.lr * p.grad) part. This is the essence of SGD. Notice that we are returning the updated parameters in a list. This is not done in the actual implementation, and instead everything is updated in place. Otherwise this is effectively the same as the Fastai source code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_sgd=OurSGD(m.parameters(),lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_parameters=our_sgd.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Please try to update the array on the left, in order to get them to be equal to our_parameters. Pay attention to how sgd is implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "parameters_equal([p for p in m.parameters()],our_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, running Fastai's optimizer will update the weights. So you will have to rerun the above to get the above problem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we do a comparison to Fastai's implmentation, just to make sure we are getting the same values. We use all_close here, because later on there will start to be slight variations as more math (and therefore error ! ) is introduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def parameters_equal(mps,ops):\n",
    "    for mp,op in zip((mps),ops):\n",
    "        print(mp.allclose(op))\n",
    "parameters_equal(m.parameters(),our_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SGD with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,labels = dls.one_batch()\n",
    "x,labels=x.cpu(),labels.cpu()\n",
    "m = nn.Sequential(nn.Conv2d(3,32,7,3,3),nn.Flatten(),nn.Linear(3200,2))\n",
    "l = nn.CrossEntropyLoss()\n",
    "lr=0.1\n",
    "mom=0.9\n",
    "opt=SGD(m.parameters(),lr,mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=m(x)\n",
    "loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurSGDwithMomentum:\n",
    "    def __init__(self,params,lr,mom):\n",
    "        self.params,self.lr=list(params),lr\n",
    "        self.mom=mom ##added\n",
    "        self.avg_grad=[torch.zeros_like(p) for p in self.params] #notice how avg_Grad starts at 0. \n",
    "    def step(self):\n",
    "        updated_params=[]\n",
    "        for i,p in enumerate(self.params):\n",
    "            updated_params.append(p.add(-self.lr*self.mom_grad(i,p.grad)))\n",
    "        return updated_params\n",
    "    #avg_grad is weighted average using momentum\n",
    "    def mom_grad(self,i,grad):\n",
    "        self.avg_grad[i]=self.mom*self.avg_grad[i]+grad #this is the important part\n",
    "        return self.avg_grad[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we add momentum, it is important to realize that momentum is a weighted average and not the \"mean.\" This weighted average is used a bit in machine learning, so it is good to get this concept down now. Momentum is one of the more important hyper parameters after learning rate and weight decay(coming up next). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_sgd=OurSGDwithMomentum(m.parameters(),lr,mom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_parameters=our_sgd.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2, make this one work as in Q1. Pay attention to how momentum works in the code above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "parameters_equal([p-lr*(mom+p.grad) for p in m.parameters()],our_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "parameters_equal(m.parameters(),our_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for step #2! We do a second step, as there is \"state\" within momentum, and we need to make sure that this state carries over to the second step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Because of state changes, update the code here to get the correct answer below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_answer=[p-lr*(our_sgd.avg_grad[i]+p.grad) for i,p in enumerate(m.parameters())] #loops through avg_grad now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_parameters=our_sgd.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "parameters_equal(our_answer,our_parameters) #this is testing your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "parameters_equal(m.parameters(),our_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SGD with Weight Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SGD Weight Decay and l2_Regularization are effectively the same. One on the weights, one on the gradients. This is not the same for more complicated optimizers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,labels = dls.one_batch()\n",
    "x,labels=x.cpu(),labels.cpu()\n",
    "m = nn.Sequential(nn.Conv2d(3,32,7,3,3),nn.Flatten(),nn.Linear(3200,2))\n",
    "l = nn.CrossEntropyLoss()\n",
    "lr=0.1\n",
    "mom=0.9\n",
    "wd=0.01\n",
    "opt=SGD(m.parameters(),lr,mom,wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=m(x)\n",
    "loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self,params,lr,mom):\n",
    "        self.mom=mom\n",
    "        self.params=params\n",
    "        self.avg_grads=[torch.zeros_like(p) for p in self.params] #avg_grad is weighted average using momentum\n",
    "    def __call__(self,**kwargs):\n",
    "        self.avg_grads = [ self.mom*avg_grad+p.grad for p,avg_grad in zip(self.params,self.avg_grads) ]\n",
    "        return {'avg_grads': self.avg_grads,**kwargs}\n",
    "class Weight_Decay:\n",
    "    def __init__(self,params,lr,wd):\n",
    "        self.lr=lr\n",
    "        self.wd=wd\n",
    "        self.params=params\n",
    "    def __call__(self,**kwargs):\n",
    "        return {**kwargs,'params':[p*(1-self.lr*self.wd) for p in self.params]} #same as params-lr*wd*params, important part!!!!\n",
    "class OurSGD:\n",
    "    def __init__(self,params,lr,mom,wd):\n",
    "        self.params,self.lr=list(params),lr\n",
    "        self.mom=Momentum(self.params,self.lr,mom)\n",
    "        self.wd=Weight_Decay(self.params,self.lr,wd)\n",
    "    def step(self):\n",
    "        updated_params=[]\n",
    "        self.params=self.wd()['params']\n",
    "        avg_grads=self.mom()['avg_grads']\n",
    "        for i,p in enumerate(self.params):\n",
    "            updated_params.append(p.add(-self.lr*avg_grads[i])) \n",
    "        return updated_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, things have gotten more complicated. We now split Momentum and Weight Decay out into two seperate functions. These are optimizer callbacks in fastai. [p * (1-self.lr * self.wd) for p in self.params] is the important bit, as well as understanding the order the math is applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_sgd=OurSGD(m.parameters(),lr,mom,wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum(mom,avg_grad,p):\n",
    "    return mom*avg_grad+p.grad\n",
    "def weight_decay(wd):\n",
    "    return wd\n",
    "our_answers=[p-lr*momentum(mom,our_sgd.mom.avg_grads[i],p)-lr*weight_decay(wd) for i,p in enumerate(m.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_parameters=our_sgd.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4, make this one true by editing the weight_decay function above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "parameters_equal(our_answers,our_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "parameters_equal(m.parameters(),our_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step two..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "opt.zero_grad()\n",
    "pred=m(x)\n",
    "loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long))\n",
    "loss.backward()\n",
    "our_parameters=our_sgd.step()\n",
    "opt.step()\n",
    "parameters_equal(m.parameters(),our_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SGD with l2 reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l2 reg and weight decay have very similar effects, so no reason to use both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn([10,3,32,32])\n",
    "m = nn.Sequential(nn.Conv2d(3,32,7,3,3),nn.Flatten(),nn.Linear(3872,2))\n",
    "l = nn.CrossEntropyLoss()\n",
    "lr=0.1\n",
    "mom=0.9\n",
    "wd=0.01\n",
    "opt=SGD(m.parameters(),lr,mom,wd,decouple_wd= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=m(x)\n",
    "loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are doing a bit of refactoring here to remove momentum and weight decay specific logic here. WE also split off the sgd_specific step while we are at it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self,params=None,lr=0.0001,mom=0.9,**kwargs):\n",
    "        self.mom=mom\n",
    "        self.params=params\n",
    "        self.avg_grads=[torch.zeros_like(p) for p in self.params] #avg_grad is weighted average using momentum\n",
    "    def __call__(self,params=None,**kwargs):\n",
    "        params = self.params if params is None else params\n",
    "        self.avg_grads = [ self.mom*avg_grad+p.grad for p,avg_grad in zip(params,self.avg_grads) ]\n",
    "        return {**kwargs,'params':params,'avg_grads': self.avg_grads}\n",
    "class Weight_Decay:\n",
    "    def __init__(self,params=None,lr=0.0001,wd=0.01,decouple=True,**kwargs):\n",
    "        self.lr=lr\n",
    "        self.wd=wd\n",
    "        self.params=params\n",
    "        self.decouple=decouple\n",
    "    def __call__(self,**kwargs):\n",
    "        params = self._do_wd() if self.decouple else self._do_l2_reg()\n",
    "        return {**kwargs,'params':params}\n",
    "    def _do_wd(self,**kwargs):\n",
    "        params=[p*(1-self.lr*self.wd) for p in self.params]\n",
    "        for p,mp in zip(params,self.params):\n",
    "            p.grad=mp.grad\n",
    "        return params #same as params-lr*wd*params\n",
    "    #this one is pretty ugly \n",
    "    def _do_l2_reg(self,**kwargs):\n",
    "        params=[deepcopy(p) for p in self.params]\n",
    "        for p,mp in zip(params,self.params):\n",
    "            p.grad=mp.grad + self.wd* mp\n",
    "        return params\n",
    "class OurSGD:\n",
    "    hypers=[Weight_Decay,Momentum]\n",
    "    def __init__(self,params,lr,**kwargs):\n",
    "        self.lr=lr\n",
    "        self.params=params\n",
    "    def __call__(self,params=None,avg_grads=None,**kwargs):\n",
    "        return {**kwargs,'params':[ p.add(-self.lr*avg) for p,avg in zip(params,avg_grads) ]}\n",
    "class OurOptimizer:\n",
    "    def __init__(self,params,lr,opt,**kwargs):\n",
    "        self.state={'params':list(params),'lr':lr}\n",
    "        self.cbs=[cls(**self.state,**kwargs) for cls in [*opt.hypers,opt]]\n",
    "    def step(self):\n",
    "        state=self.state\n",
    "        for cb in self.cbs:\n",
    "            state=cb(**state)\n",
    "        return state['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_opt=OurOptimizer(m.parameters(),lr,OurSGD,decouple=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_parameters=our_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "parameters_equal(m.parameters(),our_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "opt.zero_grad()\n",
    "pred=m(x)\n",
    "loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long))\n",
    "loss.backward()\n",
    "our_parameters=our_opt.step()\n",
    "opt.step()\n",
    "parameters_equal(m.parameters(),our_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done with my refactoring. If you notice there is a issue of lots of for...loops going over the same data. In fastai each function momentum/weight_decay/sgd works on a single parameter at a time, and that is encapsulated in a single for...loop, instead of my approach of passing all the parameters to function that does the looping itself. I just got tired of refactoring at this point and decided to keep what I had.... lots of refactoring happened not in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn([10,3,32,32])\n",
    "m = nn.Sequential(nn.Conv2d(3,32,7,3,3),nn.Flatten(),nn.Linear(3872,2))\n",
    "l = nn.CrossEntropyLoss()\n",
    "lr=0.1\n",
    "mom=0.9\n",
    "wd=0.01\n",
    "sqr_mom=0.95\n",
    "opt=RMSProp(m.parameters(),lr,sqr_mom,mom,wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=m(x)\n",
    "loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self,params=None,lr=0.0001,mom=0.9,**kwargs):\n",
    "        self.mom=mom\n",
    "        self.params=params\n",
    "        self.avg_grads=[torch.zeros_like(p) for p in self.params] #avg_grad is weighted average using momentum\n",
    "    def __call__(self,params=None,**kwargs):\n",
    "        params = self.params if params is None else params\n",
    "        self.avg_grads = [ self.mom*avg_grad+p.grad for p,avg_grad in zip(params,self.avg_grads) ]\n",
    "        return {**kwargs,'params':params,'avg_grads': self.avg_grads}\n",
    "class Weight_Decay:\n",
    "    def __init__(self,params=None,lr=0.0001,wd=0.01,decouple=True,**kwargs):\n",
    "        self.lr=lr\n",
    "        self.wd=wd\n",
    "        self.params=params\n",
    "        self.decouple=decouple\n",
    "    def __call__(self,**kwargs):\n",
    "        params = self._do_wd() if self.decouple else self._do_l2_reg()\n",
    "        return {**kwargs,'params':params}\n",
    "    def _do_wd(self,**kwargs):\n",
    "        params=[p*(1-self.lr*self.wd) for p in self.params]\n",
    "        for p,mp in zip(params,self.params):\n",
    "            p.grad=mp.grad\n",
    "        return params #same as params-lr*wd*params\n",
    "    #this one is pretty ugly \n",
    "    def _do_l2_reg(self,**kwargs):\n",
    "        params=[deepcopy(p) for p in self.params]\n",
    "        for p,mp in zip(params,self.params):\n",
    "            p.grad=mp.grad + self.wd* mp\n",
    "        return params\n",
    "class OurSGD:\n",
    "    hypers=[Weight_Decay,Momentum]\n",
    "    def __init__(self,params,lr,**kwargs):\n",
    "        self.lr=lr\n",
    "        self.params=params\n",
    "    def __call__(self,params=None,avg_grads=None,**kwargs):\n",
    "        return {**kwargs,'params':[ p.add(-self.lr*avg) for p,avg in zip(params,avg_grads) ]}\n",
    "class OurOptimizer:\n",
    "    def __init__(self,params,lr,opt,**kwargs):\n",
    "        self.state={'params':list(params),'lr':lr}\n",
    "        self.cbs=[cls(**self.state,**kwargs) for cls in [*opt.hypers,opt]]\n",
    "    def step(self):\n",
    "        state=self.state\n",
    "        for cb in self.cbs:\n",
    "            state=cb(**state)\n",
    "        return state['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learning_Rate_Decay:\n",
    "    def __init__(self, params=None,sqr_mom=0.99,**kwargs):\n",
    "        self.sqr_mom=sqr_mom\n",
    "        self.sqr_avgs=[torch.zeros_like(p) for p in params]\n",
    "    def __call__(self, params=None, dampening=True, **kwargs):\n",
    "        damp = 1-sqr_mom if dampening else 1.\n",
    "        self.sqr_avgs = [sqr_avg * self.sqr_mom + damp * p.grad.data ** 2 for p,sqr_avg in zip(params,self.sqr_avgs)]\n",
    "        return { **kwargs,'params':params,'sqr_avgs':self.sqr_avgs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurRMSProp:\n",
    "    hypers=[Weight_Decay,Momentum,Learning_Rate_Decay]\n",
    "    def __init__(self,lr,params,**kwargs):\n",
    "        self.lr=lr\n",
    "        self.params=params\n",
    "    def __call__(self,params=None,avg_grads=None,eps=1e-08,sqr_avgs=None,**kwargs):\n",
    "        return {**kwargs,'params':[ p.add(-self.lr*avg/(sqr_avg**(0.5)+eps)) for p,avg,sqr_avg in zip(params,avg_grads,sqr_avgs) ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_opt=OurOptimizer(m.parameters(),lr,OurRMSProp,sqr_mom=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_parameters=our_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "parameters_equal(m.parameters(),our_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "opt.zero_grad()\n",
    "pred=m(x)\n",
    "loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long))\n",
    "loss.backward()\n",
    "our_parameters=our_opt.step()\n",
    "opt.step()\n",
    "parameters_equal(m.parameters(),our_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn([10,3,32,32])\n",
    "m = nn.Sequential(nn.Conv2d(3,32,7,3,3),nn.Flatten(),nn.Linear(3872,2))\n",
    "l = nn.CrossEntropyLoss()\n",
    "lr=0.1\n",
    "mom=0.9\n",
    "wd=0.01\n",
    "eps=1e-05\n",
    "sqr_mom=0.95\n",
    "opt=Adam(m.parameters(),lr,mom,sqr_mom,eps,wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=m(x)\n",
    "loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weight_Decay:\n",
    "    def __init__(self,params=None,lr=0.0001,wd=0.01,decouple=True,**kwargs):\n",
    "        self.lr=lr\n",
    "        self.wd=wd\n",
    "        self.params=params\n",
    "        self.decouple=decouple\n",
    "    def __call__(self,**kwargs):\n",
    "        params = self._do_wd() if self.decouple else self._do_l2_reg()\n",
    "        return {**kwargs,'params':params}\n",
    "    def _do_wd(self,**kwargs):\n",
    "        params=[p*(1-self.lr*self.wd) for p in self.params]\n",
    "        for p,mp in zip(params,self.params):\n",
    "            p.grad=mp.grad\n",
    "        return params #same as params-lr*wd*params\n",
    "    #this one is pretty ugly \n",
    "    def _do_l2_reg(self,**kwargs):\n",
    "        params=[deepcopy(p) for p in self.params]\n",
    "        for p,mp in zip(params,self.params):\n",
    "            p.grad=mp.grad + self.wd* mp\n",
    "        return params\n",
    "class OurSGD:\n",
    "    hypers=[Weight_Decay,Momentum]\n",
    "    def __init__(self,params,lr,**kwargs):\n",
    "        self.lr=lr\n",
    "        self.params=params\n",
    "    def __call__(self,params=None,avg_grads=None,**kwargs):\n",
    "        return {**kwargs,'params':[ p.add(-self.lr*avg) for p,avg in zip(params,avg_grads) ]}\n",
    "class OurOptimizer:\n",
    "    def __init__(self,params,lr,opt,**kwargs):\n",
    "        self.state={'params':list(params),'lr':lr}\n",
    "        self.cbs=[cls(**self.state,**kwargs) for cls in [*opt.hypers,opt]]\n",
    "    def step(self):\n",
    "        state=self.state\n",
    "        for cb in self.cbs:\n",
    "            state=cb(**state)\n",
    "        return state['params']\n",
    "class Learning_Rate_Decay:\n",
    "    def __init__(self, params=None,sqr_mom=0.99,**kwargs):\n",
    "        self.sqr_mom=sqr_mom\n",
    "        self.sqr_avgs=[torch.zeros_like(p) for p in params]\n",
    "    def __call__(self, params=None, dampening=True, **kwargs):\n",
    "        damp = 1-sqr_mom if dampening else 1.\n",
    "        self.sqr_avgs = [sqr_avg * self.sqr_mom + damp * p.grad.data ** 2 for p,sqr_avg in zip(params,self.sqr_avgs)]\n",
    "        return { **kwargs,'params':params,'sqr_avgs':self.sqr_avgs}\n",
    "class OurRMSProp:\n",
    "    hypers=[Weight_Decay,Momentum,Learning_Rate_Decay]\n",
    "    def __init__(self,lr,params,**kwargs):\n",
    "        self.lr=lr\n",
    "        self.params=params\n",
    "    def __call__(self,params=None,avg_grads=None,eps=1e-08,sqr_avgs=None,**kwargs):\n",
    "        return {**kwargs,'params':[ p.add(-self.lr*avg/(sqr_avg**(0.5)+eps)) for p,avg,sqr_avg in zip(params,avg_grads,sqr_avgs) ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Step:\n",
    "    def __init__(self,**kwargs):\n",
    "        self.step=0\n",
    "    def __call__(self,**kwargs):\n",
    "        self.step+=1\n",
    "        return {'step':self.step,**kwargs}\n",
    "class Momentum:\n",
    "    def __init__(self,params=None,lr=0.0001,mom=0.9,**kwargs):\n",
    "        self.mom=mom\n",
    "        self.params=params\n",
    "        self.avg_grads=[torch.zeros_like(p) for p in self.params] #avg_grad is weighted average using momentum\n",
    "    def __call__(self,params=None,**kwargs):\n",
    "        params = self.params if params is None else params\n",
    "        self.avg_grads = [ self.mom*avg_grad+(1-self.mom)*p.grad for p,avg_grad in zip(params,self.avg_grads) ]\n",
    "        return {**kwargs,'params':params,'avg_grads': self.avg_grads}\n",
    "class OurAdam:\n",
    "    hypers=[Weight_Decay,Momentum,Learning_Rate_Decay,Step]\n",
    "    def __init__(self,lr,params,mom=0.9,sqr_mom=0.99,eps=1e-08,**kwargs):\n",
    "        self.lr=lr\n",
    "        self.params=params\n",
    "        self.mom=mom\n",
    "        self.sqr_mom=sqr_mom\n",
    "        self.eps=eps\n",
    "    def __call__(self,step=1,params=None,avg_grads=None,sqr_avgs=None,**kwargs): #eps=1e-08\n",
    "        sqr_avgs=[sqr_avg/(1 - sqr_mom**step) for sqr_avg in sqr_avgs]\n",
    "        avg_grads = [avg_grad / (1 - mom**step) for avg_grad in avg_grads]\n",
    "        return {**kwargs,'params':[ p.addcdiv( -lr ,grad_avg,(sqr_avg.sqrt() + self.eps )) for p,grad_avg,sqr_avg in zip(params,avg_grads,sqr_avgs) ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_opt=OurOptimizer(m.parameters(),lr,OurAdam,eps=eps,sqr_mom=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_parameters=our_opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows parameters close to not being equal\n",
    "def parameters_equal_show(mps,ops):\n",
    "    for mp,op in zip((mps),ops):\n",
    "        print(mp.masked_select((mp-op).abs()>1e-08),op.masked_select((mp-op).abs()>1e-08))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1133,  0.1766,  0.2099,  0.1881,  0.1587,  0.2367,  0.2189,  0.2101,\n",
      "         0.1876, -0.2224, -0.2393, -0.1429,  0.2033,  0.1898, -0.1359, -0.1989,\n",
      "        -0.2205,  0.1676, -0.2199, -0.1297, -0.1014, -0.1833, -0.1241, -0.1455,\n",
      "        -0.2361,  0.0979, -0.1395, -0.1275,  0.1032,  0.1991, -0.1265, -0.2069,\n",
      "        -0.2465,  0.2102, -0.0968, -0.2100,  0.1213, -0.1896,  0.1769, -0.2089,\n",
      "        -0.1518,  0.1592,  0.2491,  0.2233, -0.2382, -0.1474,  0.1746, -0.2236,\n",
      "         0.2342, -0.1417,  0.1879, -0.1650, -0.2419, -0.1232, -0.2413, -0.2258,\n",
      "         0.1095, -0.1980, -0.1882, -0.1661, -0.1769,  0.2334, -0.1878,  0.1639,\n",
      "        -0.2143,  0.1531, -0.1679,  0.2016, -0.1644,  0.1493,  0.2032, -0.1417,\n",
      "        -0.1873,  0.1333, -0.1954,  0.1659, -0.1852, -0.1532, -0.1191, -0.2032,\n",
      "         0.2201, -0.1217,  0.1954, -0.1297,  0.2288, -0.1301, -0.2084, -0.2102,\n",
      "        -0.1571, -0.1803, -0.1516,  0.2013, -0.2338, -0.2348,  0.2308, -0.1777,\n",
      "        -0.1932,  0.2142, -0.1990,  0.1751, -0.1063,  0.1684,  0.1321, -0.2281,\n",
      "         0.2266,  0.2006, -0.1572, -0.1618,  0.1587, -0.1836, -0.1399,  0.1631,\n",
      "         0.2076, -0.1600,  0.1584,  0.1980, -0.1637, -0.2027,  0.1153, -0.1639,\n",
      "         0.2354,  0.2395,  0.1963,  0.1393,  0.1422, -0.2111,  0.1376, -0.2402,\n",
      "        -0.1623,  0.2126, -0.2116,  0.1634,  0.2348, -0.1085,  0.1771,  0.1734,\n",
      "         0.2279, -0.0948,  0.2020,  0.1733,  0.1934, -0.2217, -0.1422, -0.2375,\n",
      "         0.1274,  0.0943,  0.1806, -0.2292, -0.2484,  0.0879, -0.0870,  0.2374,\n",
      "        -0.2179,  0.1361, -0.2142,  0.1657,  0.2259,  0.1768, -0.1184,  0.1224,\n",
      "         0.1537,  0.2078, -0.1869,  0.1412, -0.1900,  0.2006,  0.1538, -0.2469,\n",
      "         0.1525,  0.1741, -0.1754, -0.1602,  0.1738, -0.1929, -0.2245, -0.1532,\n",
      "         0.2110, -0.1917, -0.2124, -0.1408,  0.2142,  0.1549,  0.2338,  0.1441,\n",
      "         0.1377, -0.0856,  0.1355,  0.1945,  0.1407, -0.1251, -0.1614, -0.1672,\n",
      "        -0.1015, -0.2143, -0.2052, -0.1670, -0.1607, -0.1440,  0.1313, -0.2089,\n",
      "        -0.1839,  0.1929, -0.1760, -0.1672,  0.1439,  0.2235,  0.1692, -0.2194,\n",
      "         0.2251,  0.2327,  0.1336,  0.2163,  0.1336,  0.1259, -0.1727,  0.2150,\n",
      "         0.1043, -0.2463, -0.1123, -0.2426, -0.1792, -0.2002,  0.1751,  0.2148,\n",
      "         0.1448, -0.1380, -0.1563, -0.1871,  0.1831, -0.2145,  0.1607, -0.1265,\n",
      "        -0.1911,  0.1194, -0.2165,  0.1499, -0.2241,  0.2112, -0.2148,  0.2091,\n",
      "        -0.1458, -0.1751, -0.1461, -0.0967, -0.2017,  0.2188, -0.2264, -0.1651,\n",
      "         0.1476,  0.1527,  0.1455,  0.1243, -0.1918,  0.2038,  0.1374, -0.1014,\n",
      "         0.1878,  0.1480,  0.2019, -0.2465,  0.1896, -0.0874,  0.1408,  0.1890,\n",
      "        -0.1063, -0.1180,  0.2152, -0.2209, -0.0888, -0.1289, -0.0962, -0.1288,\n",
      "        -0.1128, -0.2351, -0.1592,  0.2114, -0.1509,  0.1655,  0.1508, -0.2424,\n",
      "         0.1167, -0.1288,  0.1192,  0.2172,  0.2097,  0.1360,  0.1987,  0.1679,\n",
      "        -0.2080,  0.1977,  0.1119, -0.2058,  0.2321, -0.2175,  0.1355, -0.1258,\n",
      "         0.2299, -0.1452,  0.1001,  0.2152,  0.1421,  0.1147,  0.2330, -0.1812,\n",
      "        -0.2164, -0.1955, -0.2077,  0.0908, -0.1649,  0.1881, -0.1824,  0.2457,\n",
      "        -0.2432,  0.1121, -0.1637, -0.2450,  0.1257, -0.1627,  0.1748,  0.2427,\n",
      "        -0.1599, -0.1931, -0.1066,  0.1651, -0.1312, -0.1043, -0.2015, -0.1110,\n",
      "         0.1249,  0.1858,  0.2090,  0.1888, -0.1954, -0.2169,  0.2114,  0.1973,\n",
      "        -0.2127, -0.2306,  0.2417, -0.2018,  0.1643, -0.1919,  0.1861, -0.2181,\n",
      "         0.1240,  0.1651, -0.1849,  0.1898,  0.2095, -0.1603,  0.2033,  0.1576,\n",
      "         0.2191, -0.2073,  0.1760, -0.2221,  0.2012, -0.2427, -0.2261, -0.2418,\n",
      "         0.0866, -0.1986,  0.1042,  0.1729, -0.2225,  0.2179, -0.2128, -0.2064,\n",
      "         0.0927, -0.1463, -0.1697, -0.1840, -0.1575, -0.1765, -0.1417, -0.2115,\n",
      "         0.2077,  0.1541, -0.1145,  0.2130, -0.2225, -0.1029, -0.1301,  0.2200,\n",
      "        -0.1071, -0.2340,  0.1184, -0.1121,  0.2498, -0.1454, -0.1599, -0.1192,\n",
      "         0.2128, -0.2047, -0.1026, -0.1550,  0.1381,  0.1884, -0.1259,  0.1918,\n",
      "        -0.1030, -0.1590, -0.2230,  0.2229, -0.2175,  0.1678, -0.2143, -0.1747,\n",
      "        -0.1552,  0.1536,  0.1499,  0.1595, -0.1967, -0.1533, -0.2196,  0.1193,\n",
      "        -0.1477,  0.2348,  0.2465, -0.2066,  0.1739, -0.1321,  0.2126,  0.1777,\n",
      "        -0.1872,  0.1361, -0.2462,  0.1694,  0.1474,  0.1672,  0.1734, -0.1947,\n",
      "         0.1702, -0.1690,  0.1698, -0.1295,  0.1401, -0.1864, -0.1566,  0.1362,\n",
      "         0.1980,  0.1747, -0.2286, -0.1648,  0.2033, -0.1179,  0.1884,  0.1921,\n",
      "        -0.2183,  0.2279, -0.2139, -0.0851,  0.2477,  0.1419, -0.1451, -0.1677,\n",
      "         0.2460,  0.2305,  0.1868, -0.2343, -0.1505,  0.2287,  0.1314,  0.2475,\n",
      "        -0.1446, -0.1734,  0.1849,  0.1168, -0.2334, -0.2158, -0.2291, -0.2386,\n",
      "         0.1709, -0.2310, -0.1924, -0.2474, -0.1943, -0.2114, -0.2292,  0.1596,\n",
      "        -0.2397,  0.2005,  0.1538, -0.2183, -0.2213,  0.1287,  0.1810,  0.1724,\n",
      "         0.1620, -0.1071,  0.1933,  0.2109, -0.2471, -0.1865,  0.2191,  0.1049,\n",
      "         0.1630,  0.1005, -0.1664,  0.1153,  0.1687, -0.0914, -0.2499,  0.1455,\n",
      "        -0.1582, -0.2422, -0.1850,  0.1949,  0.2278,  0.1610, -0.1282,  0.2283,\n",
      "         0.2352,  0.1533, -0.2019, -0.1028,  0.1885,  0.1379,  0.1058,  0.1140,\n",
      "        -0.2087, -0.1397, -0.1373,  0.2404,  0.2000, -0.1603,  0.1448, -0.2044,\n",
      "         0.1402,  0.2451,  0.1484,  0.1715,  0.2371, -0.1006,  0.1071, -0.1647,\n",
      "         0.1361, -0.1388, -0.1953, -0.2061,  0.1237, -0.2190, -0.2424,  0.2041,\n",
      "         0.2076, -0.2180, -0.1903, -0.1864,  0.1390,  0.2429,  0.1950,  0.2327,\n",
      "        -0.1784, -0.2185,  0.1452,  0.1724,  0.0899, -0.0891, -0.1403,  0.1413,\n",
      "         0.2000,  0.1347, -0.2395, -0.1211,  0.1891, -0.1858,  0.2475,  0.2388,\n",
      "        -0.1659, -0.2133,  0.2159,  0.2232,  0.1672,  0.1641, -0.2145,  0.1735,\n",
      "        -0.1986,  0.1812,  0.1669, -0.1667, -0.1812, -0.1491, -0.1656,  0.2409,\n",
      "        -0.1922,  0.1408, -0.1686,  0.2273, -0.1791, -0.1835, -0.1541, -0.1754,\n",
      "         0.2418,  0.2302,  0.2209, -0.1581, -0.1870,  0.1369,  0.2049,  0.2351,\n",
      "         0.2441,  0.2287,  0.2472, -0.1304,  0.2357,  0.1048, -0.2482,  0.1144,\n",
      "         0.1021, -0.1148,  0.1297,  0.1271,  0.1215,  0.1378],\n",
      "       grad_fn=<MaskedSelectBackward>) tensor([ 0.1133,  0.1766,  0.2099,  0.1881,  0.1587,  0.2367,  0.2189,  0.2101,\n",
      "         0.1876, -0.2224, -0.2393, -0.1429,  0.2033,  0.1898, -0.1359, -0.1989,\n",
      "        -0.2205,  0.1676, -0.2199, -0.1297, -0.1014, -0.1833, -0.1241, -0.1455,\n",
      "        -0.2361,  0.0979, -0.1395, -0.1275,  0.1032,  0.1991, -0.1265, -0.2069,\n",
      "        -0.2465,  0.2102, -0.0968, -0.2100,  0.1213, -0.1896,  0.1769, -0.2089,\n",
      "        -0.1518,  0.1592,  0.2491,  0.2233, -0.2382, -0.1474,  0.1746, -0.2236,\n",
      "         0.2342, -0.1417,  0.1879, -0.1650, -0.2419, -0.1232, -0.2413, -0.2258,\n",
      "         0.1095, -0.1980, -0.1882, -0.1661, -0.1769,  0.2334, -0.1878,  0.1639,\n",
      "        -0.2143,  0.1531, -0.1679,  0.2016, -0.1644,  0.1493,  0.2032, -0.1417,\n",
      "        -0.1873,  0.1333, -0.1954,  0.1659, -0.1852, -0.1532, -0.1191, -0.2032,\n",
      "         0.2201, -0.1217,  0.1954, -0.1297,  0.2288, -0.1301, -0.2084, -0.2102,\n",
      "        -0.1571, -0.1803, -0.1516,  0.2013, -0.2338, -0.2348,  0.2308, -0.1777,\n",
      "        -0.1932,  0.2142, -0.1990,  0.1751, -0.1063,  0.1684,  0.1321, -0.2281,\n",
      "         0.2266,  0.2006, -0.1572, -0.1618,  0.1587, -0.1836, -0.1399,  0.1631,\n",
      "         0.2076, -0.1600,  0.1584,  0.1980, -0.1637, -0.2027,  0.1153, -0.1639,\n",
      "         0.2354,  0.2395,  0.1963,  0.1393,  0.1422, -0.2111,  0.1376, -0.2402,\n",
      "        -0.1623,  0.2126, -0.2116,  0.1634,  0.2348, -0.1085,  0.1771,  0.1734,\n",
      "         0.2279, -0.0948,  0.2020,  0.1733,  0.1934, -0.2217, -0.1422, -0.2375,\n",
      "         0.1274,  0.0943,  0.1806, -0.2292, -0.2484,  0.0879, -0.0870,  0.2374,\n",
      "        -0.2179,  0.1361, -0.2142,  0.1657,  0.2259,  0.1768, -0.1184,  0.1224,\n",
      "         0.1537,  0.2078, -0.1869,  0.1412, -0.1900,  0.2006,  0.1538, -0.2469,\n",
      "         0.1525,  0.1741, -0.1754, -0.1602,  0.1738, -0.1929, -0.2245, -0.1532,\n",
      "         0.2110, -0.1917, -0.2124, -0.1408,  0.2142,  0.1549,  0.2338,  0.1441,\n",
      "         0.1377, -0.0856,  0.1355,  0.1945,  0.1407, -0.1251, -0.1614, -0.1672,\n",
      "        -0.1015, -0.2143, -0.2052, -0.1670, -0.1607, -0.1440,  0.1313, -0.2089,\n",
      "        -0.1839,  0.1929, -0.1760, -0.1672,  0.1439,  0.2235,  0.1692, -0.2194,\n",
      "         0.2251,  0.2327,  0.1336,  0.2163,  0.1336,  0.1259, -0.1727,  0.2150,\n",
      "         0.1043, -0.2463, -0.1123, -0.2426, -0.1792, -0.2002,  0.1751,  0.2148,\n",
      "         0.1448, -0.1380, -0.1563, -0.1871,  0.1831, -0.2145,  0.1607, -0.1265,\n",
      "        -0.1911,  0.1194, -0.2165,  0.1499, -0.2241,  0.2112, -0.2148,  0.2091,\n",
      "        -0.1458, -0.1751, -0.1461, -0.0967, -0.2017,  0.2188, -0.2264, -0.1651,\n",
      "         0.1476,  0.1527,  0.1455,  0.1243, -0.1918,  0.2038,  0.1374, -0.1014,\n",
      "         0.1878,  0.1480,  0.2019, -0.2465,  0.1896, -0.0874,  0.1408,  0.1890,\n",
      "        -0.1063, -0.1180,  0.2152, -0.2209, -0.0888, -0.1289, -0.0962, -0.1288,\n",
      "        -0.1128, -0.2351, -0.1592,  0.2114, -0.1509,  0.1655,  0.1508, -0.2424,\n",
      "         0.1167, -0.1288,  0.1192,  0.2172,  0.2097,  0.1360,  0.1987,  0.1679,\n",
      "        -0.2080,  0.1977,  0.1119, -0.2058,  0.2321, -0.2175,  0.1355, -0.1258,\n",
      "         0.2299, -0.1452,  0.1001,  0.2152,  0.1421,  0.1147,  0.2330, -0.1812,\n",
      "        -0.2164, -0.1955, -0.2077,  0.0908, -0.1649,  0.1881, -0.1824,  0.2457,\n",
      "        -0.2432,  0.1121, -0.1637, -0.2450,  0.1257, -0.1627,  0.1748,  0.2427,\n",
      "        -0.1599, -0.1931, -0.1066,  0.1651, -0.1312, -0.1043, -0.2015, -0.1110,\n",
      "         0.1249,  0.1858,  0.2090,  0.1888, -0.1954, -0.2169,  0.2114,  0.1973,\n",
      "        -0.2127, -0.2306,  0.2417, -0.2018,  0.1643, -0.1919,  0.1861, -0.2181,\n",
      "         0.1240,  0.1651, -0.1849,  0.1898,  0.2095, -0.1603,  0.2033,  0.1576,\n",
      "         0.2191, -0.2073,  0.1760, -0.2221,  0.2012, -0.2427, -0.2261, -0.2418,\n",
      "         0.0866, -0.1986,  0.1042,  0.1729, -0.2225,  0.2179, -0.2128, -0.2064,\n",
      "         0.0927, -0.1463, -0.1697, -0.1840, -0.1575, -0.1765, -0.1417, -0.2115,\n",
      "         0.2077,  0.1541, -0.1145,  0.2130, -0.2225, -0.1029, -0.1301,  0.2200,\n",
      "        -0.1071, -0.2340,  0.1184, -0.1121,  0.2498, -0.1454, -0.1599, -0.1192,\n",
      "         0.2128, -0.2047, -0.1026, -0.1550,  0.1381,  0.1884, -0.1259,  0.1918,\n",
      "        -0.1030, -0.1590, -0.2230,  0.2229, -0.2175,  0.1678, -0.2143, -0.1747,\n",
      "        -0.1552,  0.1536,  0.1499,  0.1595, -0.1967, -0.1533, -0.2196,  0.1193,\n",
      "        -0.1477,  0.2348,  0.2465, -0.2066,  0.1739, -0.1321,  0.2126,  0.1777,\n",
      "        -0.1872,  0.1361, -0.2462,  0.1694,  0.1474,  0.1672,  0.1734, -0.1947,\n",
      "         0.1702, -0.1690,  0.1698, -0.1295,  0.1401, -0.1864, -0.1566,  0.1362,\n",
      "         0.1980,  0.1747, -0.2286, -0.1648,  0.2033, -0.1179,  0.1884,  0.1921,\n",
      "        -0.2183,  0.2279, -0.2139, -0.0851,  0.2477,  0.1419, -0.1451, -0.1677,\n",
      "         0.2460,  0.2305,  0.1868, -0.2343, -0.1505,  0.2287,  0.1314,  0.2475,\n",
      "        -0.1446, -0.1734,  0.1849,  0.1168, -0.2334, -0.2158, -0.2291, -0.2386,\n",
      "         0.1709, -0.2310, -0.1924, -0.2474, -0.1943, -0.2114, -0.2292,  0.1596,\n",
      "        -0.2397,  0.2005,  0.1538, -0.2183, -0.2213,  0.1287,  0.1810,  0.1724,\n",
      "         0.1620, -0.1071,  0.1933,  0.2109, -0.2471, -0.1865,  0.2191,  0.1049,\n",
      "         0.1630,  0.1005, -0.1664,  0.1153,  0.1687, -0.0914, -0.2499,  0.1455,\n",
      "        -0.1582, -0.2422, -0.1850,  0.1949,  0.2278,  0.1610, -0.1282,  0.2283,\n",
      "         0.2352,  0.1533, -0.2019, -0.1028,  0.1885,  0.1379,  0.1058,  0.1140,\n",
      "        -0.2087, -0.1397, -0.1373,  0.2404,  0.2000, -0.1603,  0.1448, -0.2044,\n",
      "         0.1402,  0.2451,  0.1484,  0.1715,  0.2371, -0.1006,  0.1071, -0.1647,\n",
      "         0.1361, -0.1388, -0.1953, -0.2061,  0.1237, -0.2190, -0.2424,  0.2041,\n",
      "         0.2076, -0.2180, -0.1903, -0.1864,  0.1390,  0.2429,  0.1950,  0.2327,\n",
      "        -0.1784, -0.2185,  0.1452,  0.1724,  0.0899, -0.0891, -0.1403,  0.1413,\n",
      "         0.2000,  0.1347, -0.2395, -0.1211,  0.1891, -0.1858,  0.2475,  0.2388,\n",
      "        -0.1659, -0.2133,  0.2159,  0.2232,  0.1672,  0.1641, -0.2145,  0.1735,\n",
      "        -0.1986,  0.1812,  0.1669, -0.1667, -0.1812, -0.1491, -0.1656,  0.2409,\n",
      "        -0.1922,  0.1408, -0.1686,  0.2273, -0.1791, -0.1835, -0.1541, -0.1754,\n",
      "         0.2418,  0.2302,  0.2209, -0.1581, -0.1870,  0.1369,  0.2049,  0.2351,\n",
      "         0.2441,  0.2287,  0.2472, -0.1304,  0.2357,  0.1048, -0.2482,  0.1144,\n",
      "         0.1021, -0.1148,  0.1297,  0.1271,  0.1215,  0.1378],\n",
      "       grad_fn=<MaskedSelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "parameters_equal_show(m.parameters(),our_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "parameters_equal(m.parameters(),our_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "opt.zero_grad()\n",
    "pred=m(x)\n",
    "loss=l(pred,torch.zeros([pred.size()[0]],dtype=torch.long))\n",
    "loss.backward()\n",
    "our_parameters=our_opt.step()\n",
    "opt.step()\n",
    "parameters_equal(m.parameters(),our_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#1) [{'wd': 0.01, 'sqr_mom': 0.95, 'lr': 0.1, 'mom': 0.9, 'eps': 1e-05}]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataBlock??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai2)",
   "language": "python",
   "name": "fastai2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
